---
title: "Mixed Effects Models Mini-Series. Part II. Violation of independence and mixed effects"
author: "Carsten F. Dormann"
date: "27 November 2014"
output:
  html_document:
    fig_width: 5
    keep_md: no
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
---

This session will explain, why we cannot solve all statistical problems with a regression model. Specifically, we will introduce the mixed-effect model as a *hierarchical model*, in which some effects are estimated as "a different level" than the typical regression coefficients.

The key terms are:

 * non-independence of observations
 * modelling variances
 * random effects vs fixed effects
 * nested data, repeated measurements
 * autocorrelation (spatial and temporal)

One way to start thinking of hierarchical models is by example. Let's start with a very common experimental design, and what it may tell us.

# Randomised block design

In a randomised block design one set of treatment manipulations is called a block, and the block is then the unit that is replicated (with treatment units randomly shuffled within the block). The result looks like this:

![bla](MixedEffectModel_miniseries_2_files/randomisedblockfig.png)

We see that we have four treatments (in different shades of grey) and 20 replicated blocks. As an example, we shall look at the effect of competition and nematode grazing on the growth of *Artemisia maritima*. We thus have a 2 x 2 factorial design, with 16 block and missing data. We can analyse this kind of data using a classical ANOVA.

```{r}
setwd("/Users/cdormann/Data/aktuell/Misc/Stats/APES/MixedEffectModels/")
nema <- read.table("MixedEffectModel_miniseries_2_files/nematode.txt", header=TRUE)
boxplot(Artemisia ~ comp*nematodes, data=nema, col=c("white", "grey"), las=1, names=c("00", "C0", "0N", "CN"), ylab="biomass Artemisia [g]")
legend("topright",bty="n", legend=c("C = competition", "N = nematode grazing"))
summary(aov(Artemisia ~ comp*nematodes, data=nema))
```

That is fine as far as it goes. But now consider that we have thrown away information: that there are blocks. Imagine that some blocks are on wetter soil than others. In this case we would expect that there is some variability between blocks, irrespective of the treatments. Could this variability somehow be harnassed and incorporated in the model?

One approach would be to include `block` as a term in the ANOVA:

```{r}
summary(faov <- aov(Artemisia ~ as.factor(block) + comp*nematodes, data=nema))
```

Why should that make a difference? Well, because the effects are tested, using the *F*-test, against the residual variation. Any effect reducing this residual variation will therefore also increase the potential for detecting effects of competition and/or nematode grazing. Our `block`-effect contributes 159.6 sum-of-squares units, and it does so at the expense of the competition effect (which now has slightly *lower* SS). However, the residual SS is substantially reduced to 205.4 (vs earlier 341.7) units. In our example, *F*-values and significances still hardly change, but the idea should be clear, that they could have changed.

But are really interested in the effect of blocks? Aren't they just a sort of nuisance term, a random variable that our design imposes on us, but which we are not really interested in? Indeed! The `block` is a **random effect**, and the really important effects (by design) are competition and nematode grazing: these are our **fixed effects**.

In other words: Fixed effects are the model terms we are interested in, random effects are typically *structural* variables (imposed, e.g. by the design of the experiment). Think of a medical trial of a new drug, tested on various subjects. The `subject` is not really of any concern to us, it could be this person or another. It is, in a real sense, random. So if a variable represents a random selection from a large pool of things (subjects, points in the landscape, sampling intervals in time, ...), then very often we are not interested in *this specific datum*, but they represent a randomly choosen variable realisation. These terms are called **random effects**, and we shall see more of them.

There is little change, in terms of R syntax, when moving from the above ANOVA to a model, where the random effects are declared as such. A model that has both fixed and random effects is called a **mixed effects model**. It is customary to abbreviate it *LME*, not MEM, because the model is typically a **linear mixed effect model**. Let's have one:

```{r}
library(nlme)
anova(flme <- lme(Artemisia ~ comp*nematodes, random=~1|as.factor(block), data=nema, na.action=na.omit))
```

The random term is somewhat strange in structure: it can be verbally presented as: "fit a model where block is a random effect which is added as intercept". Or: a model with a random intercept effect of block. (We could also have more complicated random effects, but that is a later topic.)

One immediately obvious thing is that the output is not entirely unlike the previous ANOVA-table, but now with two rows of degrees of freedom (DF). The first is called "numerator" the second "denominator" degrees of freedom. We will see, with the next example, why we may need both of them, and that they can differ from one effect to the next.

First, however, we shall compare the fixed effect model of block with the random effect model of block in terms of model fit, using AIC:

```{r}
AIC(faov)
AIC(flme)
```

So the random effect model has a lower AIC i.e. better fit. But is that really correct? Let us look at the actual log-likelihood of the two models:

```{r}
logLik(faov)
logLik(flme)
```

Now the mixed effect model has a lower log-likelihood, i.e. poorer fit. The reason why the AIC is still better is because estimating the random effects is not counted as sucking up degrees of freedom! Thus, the fixed effect model consumes 20 degrees of freedom, while the LME only takes up 6 in terms of model complexity. The denDF still shows 32 for the LME, meaning that for the *F*-test the random factor estimates are counted. 

>  This is one of the unresolved issues in mixed effect modelling! How many degrees of freedom are taken up, and how will they be used to compute AIC and alike (see also [http://glmm.wikidot.com/faq])?
  
Let's move on to looking at the LME using the `summary` function, which is the more common approach, as the `anova` only works for Gaussian data.

```{r}
summary(flme)
```

We get all the information we would expect from a linear model, too, plus the correlation matrix of the parameter estimates (see last session) **and** the estimate of the random effects. What are these?

Okay, now it is time to explain a bit more how a random effect actually works. In our design above, the 20 blocks are actually interpreted in the LME as *random realisations* of an underlying normally distributed effect of block. That means, any block could have a higher or lower biomass value, and this value is drawn randomly from a normal distribution. The mean of this normal distribution is 0 (because it is fitted into the intercept: if it were, say, 5, then the intercept would be estimated 5 units higher). But the standard deviation of that random effect is estimated as part of the model fitting, and it is actually given in the `Random effects:` output. In this case, the standard deviation of the block effect-distribution is 1.024. The `Residual` term is the equivalent of the mean sum of squares of the residuals term in the ANOVA table, just here it is in standard deviation, there in sum of squares (2.48^2=6.15 is not very far from the 6.5 in the `faov` summary above).

> This is another important message from mixed effect models. Not only are the means for each block estimated, as in the fixed-effect model, but they are also assumed to come from the same underlying normal distribution with a standard deviation estimated alongside the LME.

There is another commonly employed package to run such mixed effect models, `lme4`. The syntax is slightly different, the output even more so, and the philosophy dramatically so.

```{r}
library(lme4)
summary(flmer <- lmer(Artemisia ~ comp*nematodes + (1|block), data=nema))
AIC(flmer)
```

First, we see no *P*-values behind the effects. Then there is no AIC and alike. Why not?

Well, statistician Douglas Bates, the main developer behind `lme4`, argues that there is no clear solution to the question of how many degrees of freedom a random effect term consumes, and hence we do not know how to compute *F*-values and *P*-values (which require the DF to compute the mean sum of squares).

To get something akin to *P*-values, we have to compute approximate confidence intervals (from within the `lme4`) or the so-called Satterthwaite approximation (also used in `nlme`):

```{r}
confint.merMod(flmer)
library(car)
Anova(flmer)
```

Just for completeness, the `MCMCglmm` package also offers an LME-model, as does `mgcv`:

```{r}
library(MCMCglmm)
fMCMCglmm <- MCMCglmm(Artemisia ~ comp*nematodes, random=~block, data=nema)
summary(fMCMCglmm)
library(mgcv)
fgamm <- gamm(Artemisia ~ comp*nematodes, random=list("block"=~1), data=nema)
summary(fgamm$lme)
```

To wrap up this first example:

 1. Mixed-effect models have fixed and random effects.
 22929. Random effects are often structural, design features or nuisance variables, while the fixed effects are what we are more interested in.
 27. Random effects are estimated differently to fixed effects, as their estimates are assumed to come from the same underlying normal distribution, which is automatically fitted.
 18. Because the random effect distribution is not estimated from the data directly, but rather from the parameter fitted to blocks, it is at a different level, one level up (or down, depending on how you look at it) in the hierarchy. Thus, random effect models are **hierarchical models**, with parameter estimated at different *distances* from the data.
 2992112. The hierarchical structure has an effect on the estimation of the model parameters, as well as on the likelihood. Sometimes, this produces rather strange effects that even statisticians struggle to understand. 
 
 
#  Nested sampling designs

Our next example is a bit more realistic for the use mixed effect models. Here the design is different in that several values are measured per experimental unit. In this specific case (see figure), we also have blocks as random effects, which we will (initially) ignore.

![bla](MixedEffectModel_miniseries_2_files/nesteddesign.png)

In the example data, we have measured in each point the sex of an individual of arctic willow (*Salix polaris*). Treatment is herbivore exclosure, with controls allowing free access. The research question is: Does the exclusion of herbivores lead to changes is the sex-ratio of the preferred food plant?

The data look like this:

```{r}
setwd("/Users/cdormann/Data/aktuell/Misc/Stats/APES/MixedEffectModels/")
will <- read.table("MixedEffectModel_miniseries_2_files/willowsex.txt", header=TRUE)
head(will, 12)
```

`blocks` are the replicated unit (in a randomised block design with 6 replicates), `subs` refers to the subsample (1 to 10), i.e. the actual measurement within each sampling unit, `excl` is the treatment (0 and 1), and `female` is the response, with a 1 indicating a female plant and a 0 indicating a male.

It should be obvious (?!) that we do **not** have 120 independent measurements! We have only 6 blocks with 2 treatments each, i.e. 12 experimental units, no matter how often we subsample any of them. Thus, **by design** we are forced to somehow accomodate the fact that we have subsamples.

The traditional (and correct but inefficient and sometimes more difficult) way of dealing with this is to average the subsamples. In our case, we could compute the percentage of females in each of the 12 plots. However, percentages are typically not nicely distributed and may require a GLM with some fancy distribution (such as the beta). Instead, we want to use the actually observed data, which are binary, requiring us to use a binomial GLM.

First, we plot the data, using the traditional method of averaging subplots:

```{r}
attach(will)
averaged <- tapply(female, list(block, excl), mean)
par(mar=c(5,5,1,1))
boxplot(averaged, names=c("control", "exclosure"), las=1, col="grey", ylim=c(0,1), ylab="proportion female")
detach(will)
```

So, here we use the random part of the mixed effect model to instruct the `lme`, how to correctly use the subsample information, like so:

```{r}
fglmer <- glmer(female ~ excl + (1|block/excl/subs), family=binomial, data=will)
summary(fglmer)
```

The random-code tells the model that the subsamples are *nested* within the actual treatment (`excl`), which are themselves nested in blocks, as indicated by the `/`.

To run the same analysis with the `lme`-equivalent function, we resort to the following code:

```{r}
library(MASS)
fglmmPQL <- glmmPQL(female ~ excl, random=~1|block/excl/subs, family=binomial, data=will)
summary(fglmmPQL)
```
<!---
fgamm2 <- gamm(female ~ excl, random=list("block"= ~excl, "excl"=~subs), data=will, family=binomial)
summary(fgamm2$lme)
-->

The results differ slightly, but not qualitatively. (Don't ask me why the `glmer` does give *P*-values, althought the `lmer` did not.) More interestingly, the variance estimated for the random effects are virtually all approximately 0. This is often the case when the random effects are "only" structural constraints. It means that there is no relevant variance stored in these levels, and that the majority of effects is still in the fixed effect (and residuals).

You can (and should!) check that the model was correctly specified by looking at the degrees of freedom. Remembering our experience from the previous analysis, we expect as the remaining DF the number of plots (not subsamples) minus the number of estimated effects (without intercept) minus the number of random effect parameters *at the level above the plot* (in this case 6 blocks): 12 - 1 - 6 = 5. This is indeed the DF-value in the `glmmPQL`-output (but not provided by the `glmer`). Personally, I find the `glmer`-output not sufficient to see whether I have coded the model correctly.



<!---
Finally, we may want to plot the predictions for exclosures from this model. This is a bit tricky, since the model requires the specification of exactly which block (and subsample) the prediction should be for. But we want to know the effect irrespective of the random effects. (We can do so using the function `lsmeans` of the support-package `lmerTest` but *only for linear mixed effect models*, not their generalised equivalent). Instead, we have to compute the prediction for each block separately and then average:

 # fine, but only yields means, which we can easily extract from the model directly
preds <- predict(fglmer, newdata=data.frame(excl=rep(c(0,1), each=6), block=rep(1:6, times=2), subs=c(1,1)))
tapply(preds, rep(c(0,1), each=6), mean)
-->
<!---
 doesn't work: for lmer only!
library(lmerTest)
lsmeans(fglmer, test.effs="excl")
-->

<!---
response: listmafragh$traits$tabQuantitative
predictor: listmafragh$env
phylogeny: listmafragh$phy (phylog object)
aus: http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2745.2010.01743.x/suppinfo
-->


Outlook:


At the heart of mixed effect models lie two related concepts: 

 1. Accommodating non-independence of data points.
 2. Estimating model parameters at different hierarchical levels.
 
 
There are other sources of non-independence than the two designs discussed above. Consider these examples:
 
 1. Bark thickness is measured in three tree species at 1, 3, 5 and 10 m height, in 20 replicate trees per species. What is the the random effect? What would the model look like?
 2. Body size of male and female mammals is compared between asian and american species. Where is the non-independence? How can we correct for it?
 3. Every year for ten years, the wristbones of children starting at age 8 are measured. We want to reconstruct phases of growth in girls and boys. Where is the non-independence?
 4. The geographic distribution of a forest pest outbreak is analysed in a GIS for its potential environmental predictors. Pixel size is 1 ha. Where is the (potential) problem?
 5. Racoon population size in GÃ¶ttingen has been monitored since over 20 years now. If we want to analyse how population size depends on the increase of maize production, where may the problem of non-independence come in?
 
 
These examples show that non-independence may arise phylogenetically, spatially, temporally and by experimental design. We must practice to detect non-independence by design, but luckily we can diagnose spatial/temporal/phylogenetic independence in data from surveys, where the design does not necessarily suggest a problem. More on this in the next session.
  

